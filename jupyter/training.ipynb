{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import cycle\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from python.models.models import DeepGenerativeModel\n",
    "from python.models.variational import SVI, ImportanceWeightedSampler\n",
    "from python.models.utils import binary_cross_entropy, ikatura_saito_divergence\n",
    "\n",
    "from python.data import SpectrogramLabeledFrames\n",
    "\n",
    "# Settings\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "## Deep Generative Model\n",
    "x_dim = 513 # frequency bins (spectrogram)\n",
    "#y_dim = 1 # frequency bins (binary mask)\n",
    "y_dim = 513 # frequency bins (binary mask)\n",
    "z_dim = 128\n",
    "h_dim = [256, 128]\n",
    "\n",
    "## Loss\n",
    "alpha = 0.1\n",
    "\n",
    "## Training\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "log_interval = 1\n",
    "start_epoch = 1\n",
    "end_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "- Number of training samples: 972\n",
      "- Number of validation samples: 976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/jrichter/.local/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-2-318382f10829>(90)<module>()\n",
      "-> classication_loss = -torch.sum(y*torch.log(y_hat + 1e-8) + \\\n",
      "(Pdb) accuracy\n",
      "0.145610278372591\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = DeepGenerativeModel([x_dim, y_dim, z_dim, h_dim])\n",
    "if cuda: model = model.cuda()\n",
    "\n",
    "\n",
    "# Load data\n",
    "print('Load data')\n",
    "train_data = pickle.load(open('../data/subset/processed/si_tr_s_frames.p', 'rb'))\n",
    "valid_data = pickle.load(open('../data/subset/processed/si_dt_05_frames.p', 'rb'))\n",
    "\n",
    "train_labels = pickle.load(open('../data/subset/processed/si_tr_s_labels.p', 'rb'))\n",
    "valid_labels = pickle.load(open('../data/subset/processed/si_dt_05_labels.p', 'rb'))\n",
    "\n",
    "# Dataset class\n",
    "train_dataset = SpectrogramLabeledFrames(train_data, train_labels)\n",
    "valid_dataset = SpectrogramLabeledFrames(valid_data, valid_labels)\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, sampler=None, \n",
    "                        batch_sampler=None, num_workers=0, pin_memory=False, \n",
    "                        drop_last=False, timeout=0, worker_init_fn=None)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, sampler=None, \n",
    "batch_sampler=None, num_workers=0, pin_memory=False, \n",
    "                        drop_last=False, timeout=0, worker_init_fn=None)\n",
    "\n",
    "print('- Number of training samples: {}'.format(len(train_dataset)))\n",
    "print('- Number of validation samples: {}'.format(len(valid_dataset)))\n",
    "\n",
    "\n",
    "# Optimizer settings\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "\n",
    "# We can use importance weighted samples [Burda, 2015] to get a better estimate\n",
    "# on the log-likelihood.\n",
    "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
    "\n",
    "#elbo = SVI(model, likelihood=binary_cross_entropy, sampler=sampler)\n",
    "elbo = SVI(model, likelihood=ikatura_saito_divergence, sampler=sampler)\n",
    "\n",
    "\n",
    "\n",
    "def F1_score(y, y_hat):\n",
    "    TP = 0.0\n",
    "    FP = 0.0\n",
    "    TN = 0.0\n",
    "    FN = 0.0\n",
    "\n",
    "    for j in range(y_hat.shape[0]):\n",
    "        for i in range(y_hat.shape[1]): \n",
    "            if y[j][i]==y_hat[j][i]==1:\n",
    "                TP += 1.0\n",
    "            if y_hat[j][i]==1 and y[j][i]!=y_hat[j][i]:\n",
    "                FP += 1.0\n",
    "            if y[j][i]==y_hat[j][i]==0:\n",
    "                TN += 1.0\n",
    "            if y_hat[j][i]==0 and y[j][i]!=y_hat[j][i]:\n",
    "                FN += 1.0 \n",
    "                \n",
    "    F1_score = TP / (TP + 0.5*(FP+FN))\n",
    "            \n",
    "    return F1_score\n",
    "\n",
    "\n",
    "# Training\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    model.train()\n",
    "    total_loss, accuracy = (0, 0)\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "        if cuda:\n",
    "            # They need to be on the same device and be synchronized.\n",
    "            x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "        L = -elbo(x, y)\n",
    "        # U = -elbo(u)\n",
    "\n",
    "        # Add auxiliary classification loss q(y|x)\n",
    "        y_hat = model.classify(x)\n",
    "        y_seg = y_hat > 0.5\n",
    "        \n",
    "        accuracy = F1_score(y, y_seg)\n",
    "                \n",
    "        \n",
    "        pdb.set_trace()\n",
    "        \n",
    "        # Regular cross entropy\n",
    "        classication_loss = -torch.sum(y*torch.log(y_hat + 1e-8) + \\\n",
    "                                       (1.0-y)*torch.log(1.0 - y_hat + 1e-8), dim=1).mean()\n",
    "\n",
    "        J_alpha = L - alpha * classication_loss  # + U\n",
    "\n",
    "        J_alpha.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # J_alpha is a scalar, so J_alpha.data[0] does not work\n",
    "        total_loss += J_alpha.item()\n",
    "        \n",
    "        \n",
    "        accuracy += torch.mean((torch.max(y_hat, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        m = valid_dataset.data.shape[1]\n",
    "\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
    "\n",
    "        total_loss, accuracy = (0, 0)\n",
    "        for batch_idx, (x, y) in enumerate(valid_loader):\n",
    "\n",
    "            if cuda:\n",
    "                x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "            L = -elbo(x, y)\n",
    "            #U = -elbo(x)\n",
    "\n",
    "            y_hat = model.classify(x)\n",
    "            classication_loss = -torch.sum(y * torch.log(y_hat + 1e-8), dim=1).mean()\n",
    "\n",
    "            J_alpha = L + alpha * classication_loss #+ U\n",
    "\n",
    "            # J_alpha is a scalar, so J_alpha.data[0] does not work\n",
    "            total_loss += J_alpha.item()\n",
    "\n",
    "            _, pred_idx = torch.max(y_hat, 1)\n",
    "            _, lab_idx = torch.max(y, 1)\n",
    "            accuracy += torch.mean((torch.max(y_hat, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "        m = valid_dataset.data.shape[1]\n",
    "        print(\"[Validation]\\t J_a: {:.2f}, accuracy: {:.2f}\".format(total_loss / m, accuracy / m))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
